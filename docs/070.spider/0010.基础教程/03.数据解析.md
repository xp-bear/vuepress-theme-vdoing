---
title: 数据解析
date: 2023-02-07 16:45:50
permalink: /pages/c3a956/
categories:
  - spider
tags:
  - 
---
# 数据解析

## xpath

### 1.xpath的使用

注意：提前安装xpath插件

（1）打开chrome浏览器

（2）点击右上角小圆点

（3）更多工具

（4）扩展程序

（5）拖拽xpath插件到扩展程序中

（6）如果crx文件失效，需要将后缀修改zip

（7）再次拖拽

（8）关闭浏览器重新打开

（9）ctrl + shift + x

（10）出现小黑框

#### 1.安装lxml库

```python
pip install lxml ‐i https://pypi.douban.com/simple
```

#### 2.导入lxml.etree

```python
from lxml import etree
```

#### 3.etree.parse() 解析本地文件

```python
html_tree = etree.parse('XX.html')
```

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8"/>
    <title>Title</title>
</head>
<body>
<ul>
    <li id="l1" class="c1">刘德华</li>
    <li id="l2">张学友</li>
    <li id="c3">郭富城</li>
    <li id="c4">黎明</li>
    <li id="3l">周星星</li>
</ul>
</body>
</html>
```

```python
from lxml import etree

# xpath解析
# （1）本地文件                                          ---->   etree.parse()
# （2）服务器响应的数据  response.read().decode('utf-8')  ---->   etree.HTML()

# xpath解析本地文件
tree = etree.parse('./01-xpath的基本使用.html')
# print(tree)  # <lxml.etree._ElementTree object at 0x0000020661728400>


# 查找ul下面的li
# li_list = tree.xpath('//body/ul/li')

# 查找所有有id的属性的li标签
# text()获取标签中的内容
# li_list = tree.xpath('//ul/li[@id]/text()')

# 找到id为l1的li标签  注意引号的问题
# li_list = tree.xpath('//ul/li[@id="l1"]/text()')

# 查找到id为l1的li标签的class的属性值
# li_list = tree.xpath('//ul/li[@id="l1"]/@class')

# 查询id中包含l的li标签
# li_list = tree.xpath('//ul/li[contains(@id,"l")]/text()')

# 查询id的值以c开头的li标签
# li_list = tree.xpath('//ul/li[starts-with(@id,"c")]/text()')

# 查询id为l1和class为c1的li标签
# li_list = tree.xpath('//ul/li[@id="l1" and @class="c1"]/text()')

# 查询id为l1的li标签或者id为l2的li标签
li_list = tree.xpath('//ul/li[@id="l1"]/text() | //ul/li[@id="l2"]/text()')

# 判断列表的长度
print(li_list)
print(len(li_list))
```

#### 4.etree.HTML() 服务器响应文件

```python
html_tree = etree.HTML(response.read().decode('utf‐8')
```

### 2.xpath基本语法

#### 1.路径查询

`//`：查找所有子孙节点，不考虑层级关系

`/ `：找直接子节点

#### 2.谓词查询

`//div[@id]`

`//div[@id="maincontent"]`

#### 3.属性查询

`//@class`

#### 4.模糊查询

`//div[contains(@id, "he")]`

`//div[starts‐with(@id, "he")]`

#### 5.内容查询

`//div/h1/text()`

#### 6.逻辑运算

`//div[@id="head" and @class="s_down"]`

`//title | //price`



案例1: 获取百度网站的百度一下

```python
#  (1)  获取网页的源码
#  (2)  解析   解析的服务器响应的文件  etree.HTML
#  (3)  打印

import urllib.request

url = 'https://www.baidu.com/'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

# 请求对象的定制
request = urllib.request.Request(url=url, headers=headers)

# 模拟浏览器访问服务器
response = urllib.request.urlopen(request)

# 获取网页源码
content = response.read().decode('utf-8')

# 解析网页源码 来获取我们想要的数据
from lxml import etree

# 解析服务器响应的文件
tree = etree.HTML(content)

# 获取想要的数据  xpath的返回值是一个列表类型的数据
result = tree.xpath('//input[@id="su"]/@value')[0]

print(result) #百度一下
```

案例2: 抓取站长素材图片(http://sc.chinaz.com/tupian/shuaigetupian.html)

```python
#  (1) 请求对象的定制
# （2）获取网页的源码
# （3）下载


# 需求 下载的前十页的图片
# https://sc.chinaz.com/tupian/qinglvtupian.html   1
# https://sc.chinaz.com/tupian/qinglvtupian_page.html

import urllib.request
from lxml import etree
import os


def create_request(page):
    if (page == 1):
        url = 'https://sc.chinaz.com/tupian/qinglvtupian.html'
    else:
        url = 'https://sc.chinaz.com/tupian/qinglvtupian_' + str(page) + '.html'

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
    }

    request = urllib.request.Request(url=url, headers=headers)
    return request


def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content


def down_load(content):
    #     下载图片
    # 判断是否有该文件夹
    if not os.path.exists('./loveImg'):
        os.makedirs('./loveImg')

    # urllib.request.urlretrieve('图片地址','文件的名字')
    tree = etree.HTML(content)

    name_list = tree.xpath('//div[@id="container"]//a/img/@alt')

    # 一般设计图片的网站都会进行懒加载
    src_list = tree.xpath('//div[@id="container"]//a/img/@src2')

    for i in range(len(name_list)):
        name = name_list[i]
        src = src_list[i]
        url = 'https:' + src
        urllib.request.urlretrieve(url=url, filename='./loveImg/' + name + '.jpg')
        print(f'{name}.jpg--over!')


if __name__ == '__main__':
    start_page = int(input('请输入起始页码'))
    end_page = int(input('请输入结束页码'))

    for page in range(start_page, end_page + 1):
        # (1) 请求对象的定制
        request = create_request(page)
        # （2）获取网页的源码
        content = get_content(request)
        # （3）下载
        down_load(content)
```

## JsonPath

### 1.jsonpath的安装及使用方式

pip安装：

```python
pip install jsonpath
```

jsonpath的使用：

```python
obj = json.load(open('json文件', 'r', encoding='utf‐8'))
ret = jsonpath.jsonpath(obj, 'jsonpath语法')
```

教程连接（http://blog.csdn.net/luxideyao/article/details/77802389）

JSONPath - 是xpath在json的应用。jsonpath只可以解析本地的`json`文件。

如果可以使用xpath来解析json，以下的问题可以被解决：

1，数据不使用特殊的脚本，可以在客户端交互的发现并取并获取。

2，客户机请求的JSON数据可以减少到服务器上的相关部分，这样可以最大限度地减少服务器响应的带宽使用率。



xpath的表达式：

`/store/book[1]/title`

我们可以看作是：

`$.store.book[0].title`	或	`$['store']['book'][0]['title']`

### 2.JSONPath 基本语法

JSONPath 是参照，xpath表达式来解析xml文档的方式，json数据结构通常是匿名的并且**不一定需要有根元素**。==JSONPath 用一个抽象的名字$来表示最外层对象。==



JOSNPath 表达式可以使用.  符号如下：

```json
$.store.book[0].title
```

或者使用[] 符号如下:

```json
$['store']['book'][0]['title']
```



JSONPath 允许使用通配符 * 表示所有的子元素名和数组索引。

表达式在下面的脚本语言中可以使用显示的名称或者索引：

使用'@'符号表示当前的对象。

```json
$.store.book[(@.length-1)].title  
```

使用 ? (<判断表达式>) 使用逻辑表达式来过滤。

```json
$.store.book[?(@.price < 10)].title
```



**xpath和jsonpath脚本之中的不同点:**

- [ ]在xpath表达式总是从前面的路径来操作数组，索引是从1开始。
- 使用JOSNPath的[ ]操作符操作一个对象或者数组，索引是从0开始。

```json
# jsonpath.json
{
  "store": {
    "book": [
      {
        "category": "修真",
        "author": "罗翔",
        "title": "法外狂徒是怎样练成的",
        "price": 8.95
      },
      {
        "category": "修真",
        "author": "天蚕土豆",
        "title": "斗破苍穹",
        "price": 12.99
      },
      {
        "category": "修真",
        "author": "唐家三少",
        "title": "斗罗大陆",
        "isbn": "0-553-21311-3",
        "price": 8.99
      },
      {
        "category": "修真",
        "author": "南派三叔",
        "title": "星辰变",
        "isbn": "0-395-19395-8",
        "price": 22.99
      }
    ],
    "bicycle": {
      "author": "老马",
      "color": "黑色",
      "price": 19.95
    }
  }
}
```

```python
import json
import jsonpath

obj = json.load(open('jsonpath.json', 'r', encoding='utf-8'))

# 条件过滤
# name = jsonpath.jsonpath(obj, '$.store.book[(@.length-1)].title')  # 最后一本书的标题
# name = jsonpath.jsonpath(obj, '$.store.book[?(@.price < 10)].title')  # 筛选价格小于10的书籍标题
# print(name)

# 书店所有书的作者
# author_list = jsonpath.jsonpath(obj,'$.store.book[*].author')
# print(author_list)

# 所有的作者
# author_list = jsonpath.jsonpath(obj, '$..author')  # 查找json下面所有author属性的值
# print(author_list)

# store下面的所有的元素
# tag_list = jsonpath.jsonpath(obj,'$.store.*') #返回的结果用一个列表
# print(tag_list)

# store里面所有东西的price
# price_list = jsonpath.jsonpath(obj,'$.store..price')
# print(price_list)

# 拿到bicycle这一个属性下面的color属性的字段
# book = jsonpath.jsonpath(obj, "$..bicycle['color']")
# print(book)

# 获取到最后一本书
# book = jsonpath.jsonpath(obj,'$..book[(@.length-1)]')
# print(book)

# 	前面的两本书
# book_list = jsonpath.jsonpath(obj,'$..book[0,1]')
# book_list = jsonpath.jsonpath(obj,'$..book[:2]')
# print(book_list)

# 条件过滤需要在（）的前面添加一个？
# 过滤出所有的包含isbn的书。
# book_list = jsonpath.jsonpath(obj, '$..book[?(@.isbn)]')
# print(book_list)


# 哪本书超过了10块钱
# book_list = jsonpath.jsonpath(obj, '$..book[?(@.price>10)]')
# print(book_list)
```

| **XPath**              | **JSONPath**                       | **结果**                               |
| ---------------------- | ---------------------------------- | -------------------------------------- |
| `/store/book/author`   | `$.store.book[*].author`           | 书点所有书的作者                       |
| `//author`             | `$..author`                        | 所有的作者                             |
| `/store/*`             | `$.store.*`                        | store的所有元素。所有的bookst和bicycle |
| `/store//price`        | `$.store..price`                   | store里面所有东西的price               |
| `//book[3]`            | `$..book[2]`                       | 第三个书                               |
| `//book[last()]`       | `$..book[(@.length-1)]`            | 最后一本书                             |
| `//book[position()<3]` | `$..book[0,1]`  <br/>`$..book[:2]` | 前面的两本书。                         |
| `//book[isbn]`         | `$..book[?(@.isbn)]`               | 过滤出所有的包含isbn的书。             |
| `//book[price<10]`     | `$..book[?(@.price<10)]`           | 过滤出价格低于10的书。                 |
| `//*`                  | `$..*`                             | 所有元素。                             |

案例: 抓取淘票票网站城市信息

```python
import urllib.request

url = 'https://dianying.taobao.com/cityAction.json?activityId&_ksTS=1629789477003_137&jsoncallback=jsonp138&action=cityAction&n_s=new&event_submit_doGetAllRegion=true'

headers = {
    # ':authority': 'www.taopiaopiao.com',
    # ':method': 'GET',
    # ':path': '/cityAction.json?activityId&_ksTS=1643789704254_108&jsoncallback=jsonp109&action=cityAction&n_s=new&event_submit_doGetAllRegion=true',
    # ':scheme': 'https',
    'accept': 'text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01',
    # 'accept-encoding': 'gzip, deflate, br',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cookie': 't=5b297ecdc7ee59044398aba517580a86; _tb_token_=ea3ee40e5a06e; cookie2=1dc3645cd9fb9dbe75b103bc7b7e6962; cna=miZrGtm2igwCAbdeliDtswYZ; xlly_s=1; tb_city=420100; tb_cityName="zuS6ug=="; l=eBaTsc_IgqnJfwewBO5ZPurza77O3IRb4sPzaNbMiInca18dtF_ceNCp7QcWSdtjgtCnnetP6yJJoRLHR3AiBwKF_6aeVerx3xvO.; isg=BHh4lLjobSHTvYGlXgkqua9CSSYK4dxry7NzD7LpWrNmzRi3WvBs-8jjhcX9nZRD; tfstk=ceRGBQa81dW_9BXHFf16-VmA1xkRZfHNfIReTQiq4ZtBgndFij2UU1HK-GnSb-1..',
    'referer': 'https://www.taopiaopiao.com/?spm=a1z21.3046609.city.10.1d69112alzi4QQ&city=420100',
    'sec-ch-ua': '" Not A;Brand";v="99", "Chromium";v="96", "Google Chrome";v="96"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',
    'x-requested-with': 'XMLHttpRequest',
}

request = urllib.request.Request(url=url, headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode('utf-8')
print(content)

# split 切割
content = content.split('(')[1].split(')')[0]

with open('05-jsonpath解析淘票票.json', 'w', encoding='utf-8')as fp:
    fp.write(content)

import json
import jsonpath

obj = json.load(open('05-jsonpath解析淘票票.json', 'r', encoding='utf-8'))

city_list = jsonpath.jsonpath(obj, '$..regionName')

print(city_list)
print(len(city_list))
```

## BeautifulSoup

### 1.基本简介

**1.BeautifulSoup简称：** `bs4`

**2.什么是BeatifulSoup？**

BeautifulSoup，和lxml一样，是一个html的解析器，主要功能也是解析和提取数据

**3.优缺点？**

- 缺点：效率没有lxml的效率高
- 优点：接口设计人性化，使用方便

### 2.安装以及创建

1.安装

```python
pip install bs4
```

2.导入

`from bs4 import BeautifulSoup`

3.创建对象

服务器响应的文件生成对象

```python
soup = BeautifulSoup(response.read().decode(), 'lxml') #后面的lxml为固定的格式
```

本地文件生成对象

```python
soup = BeautifulSoup(open('1.html',encoding='utf-8'), 'lxml')
```

注意：默认打开文件的编码格式gbk所以需要指定打开编码格式

### 3.节点定位

#### 1.根据标签名查找节点

```python
soup.a #【注】只能找到第一个a
soup.a.name #获取到a标签name属性里面的值
soup.a.attrs #获取到a标签的属性值
```

#### 2.函数

(1)  `find(返回一个对象)`

```python
find('a') #只找到第一个a标签
find('a', title='名字') #通过条件 找到 对应的标签
find('a', class_='名字') #注意class需要添加下划线
```

(2)  `find_all(返回一个列表)`

```python
find_all('a') #查找到所有的a
find_all(['a', 'span']) #返回所有的a和span标签
find_all('a', limit=2) #只找前两个a标签
```

(3)  `select(根据选择器得到节点对象)【推荐】`

**select方法返回的是一个列表 并且会返回多个数据**。

- 1.element

  eg: `soup.select('a')  直接通过标签属性 获取标签。`  

- 2.class

  eg:  `soup.select('.firstname') 可以通过.代表class我们把这种操作叫做类选择器`

- 3.#id

  eg: `soup.select('#firstname') 可以通过#代表id我们把这种操作叫做id选择器`

- 4.属性选择器[attribute]

  eg: `li = soup.select('li[class]') #查找到1i标签中有class的标签`

  [attribute=value]
  eg: `li = soup.select('li[class="hengheng1"]')  #查找到1i标签中有class='hengheng1'的标签`

- 5.层级选择器

  element element		 `div p`  `soup = soup.select('div p')` 后代选择器
  element>element		`div>p`  `soup = soup.select('div>p')` 某标签的第一级子标签
  element,element		 `div,p`  `soup = soup.select('a,span')` 

### 4.节点信息

**(1).获取节点内容：适用于标签中嵌套标签的结构**

- `obj.string`
- `obj.get_text()`【推荐】

二者之间的区别: 

如果标签对象中只有内容那么 `string` 和`get_text（）`都可以使用

如果标签对象中除了内容还有标签那么`string`就获取不到数据而`get_text（）`是可以获取数据

**(2).节点的属性**

- `tag.name `获取标签名

```python
tag = find('li')
print(tag.name) #li
```

- `tag.attrs`将属性值作为一个字典返回

**(3).获取节点属性**

- `obj.attrs.get('title')` ==【常用】==
- `obj.get('title')`
- `obj['title']`

案例: 

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body>

<div>
    <ul>
        <li id="l1">张三</li>
        <li id="l2">李四</li>
        <li>王五</li>
        <a href="http://www.xxoutman.cn" id="" class="a1">萧兮大帅锅</a>
        <span>嘿嘿嘿</span>
    </ul>
</div>


<a href="http://www.baidu.com" title="a2">百度</a>

<div id="d1">
   <span>哈哈哈</span>
</div>

<p id="p1" class="p1">呵呵呵</p>
</body>
</html>
```

```python
from bs4 import BeautifulSoup

# 通过解析本地文件 来将bs4的基础语法进行讲解
# 默认打开的文件的编码格式是gbk 所以在打开文件的时候需要指定编码
soup = BeautifulSoup(open('./06-bs4的基本使用.html', encoding='utf-8'), 'lxml')

# 根据标签名查找节点,找到的是第一个符合条件的数据 (返回一个标签对象)
# print(soup.a)

# 获取第一个符合条件的标签的属性和属性值 (返回一个对象)
# print(soup.a.attrs)

# bs4的一些函数 (总共有3个函数)
# （1）find
# 返回的是第一个符合条件的数据(标签对象)
# print(soup.find('a'))  # <a class="a1" href="" id="">萧兮大帅锅</a>

# 根据title的值来找到对应的标签对象
# print(soup.find('a', title="a2"))  # <a href="" title="a2">百度</a>

# 根据class的值来找到对应的标签对象  注意的是class需要添加下划线
# print(soup.find('a', class_="a1"))  # <a class="a1" href="" id="">萧兮大帅锅</a>

# （2）find_all  返回的是一个列表 并且返回了所有的a标签
# print(soup.find_all('a'))

# 如果想获取的是多个标签的数据 那么需要在find_all的参数中添加的是列表的数据
# print(soup.find_all(['a','span']))

# limit的作用是查找前几个数据
# print(soup.find_all('li',limit=2))


# （3）select（推荐）
# select方法返回的是一个列表  并且会返回多个数据
# 可以通过标签名进行一个选择,我们叫做属性选择器
# print(soup.select('a'))

# 可以通过.代表class  我们把这种操作叫做类选择器
# print(soup.select('.a1'))

# 可以通过#代表id  我们把这种操作叫做id选择器
# print(soup.select('#l1'))


# 属性选择器---通过属性来寻找对应的标签
# 查找到li标签中有id的标签
# print(soup.select('li[id]'))

# 查找到li标签中id为l2的标签
# print(soup.select('li[id="l2"]'))


# 层级选择器
#  后代选择器
# 找到的是div下面的li
# print(soup.select('div li'))

# 子代选择器
#  某标签的第一级子标签
# 注意：很多的计算机编程语言中 如果不加空格不会输出内容  但是在bs4中 不会报错 会显示内容
# print(soup.select('div > ul > li'))

# 找到a标签和li标签的所有的对象
# print(soup.select('a,li'))


# 节点信息
# 获取节点内容
# obj = soup.select('#d1')[0]
# 如果标签对象中 只有内容 那么string和get_text()都可以使用
# 如果标签对象中 除了内容还有标签 那么string就获取不到数据 而get_text()是可以获取数据
# 我们一般情况下  推荐使用get_text()
# print(obj.string)
# print(obj.get_text())

# 节点的属性
# obj = soup.select('#p1')[0]
# name是标签的名字
# print(obj.name)
# 将属性值左右一个字典返回
# print(obj.attrs)

# 获取节点的属性
# obj = soup.select('#p1')[0]
# 
# print(obj.attrs.get('class'))  # 常用
# print(obj.get('class'))
# print(obj['class'])
```

案例: 抓取星巴克咖啡品种(https://www.starbucks.com.cn/menu/)

```python
import urllib.request

url = 'https://www.starbucks.com.cn/menu/'

response = urllib.request.urlopen(url)

content = response.read().decode('utf-8')

from bs4 import BeautifulSoup

soup = BeautifulSoup(content, 'lxml')

# //ul[@class="grid padded-3 product"]//strong/text()
name_list = soup.select('ul[class="grid padded-3 product"] strong')
print(name_list)
# for name in name_list:
#     print(name.get_text())
```

案例: 抓取星巴克产品图片

```python
import urllib.request
import os

url = 'https://www.starbucks.com.cn/menu/'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Accept-Language': 'zh-CN,zh;q=0.9',
    'Host': 'www.starbucks.com.cn',
}
request = urllib.request.Request(url=url, headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode('utf-8')

from bs4 import BeautifulSoup

soup = BeautifulSoup(content, 'lxml')

name_list = soup.select('ul[class="grid padded-3 product"] strong')
img_list = soup.select("div[class='preview circle']")
if not os.path.exists('./星巴克'):
    os.mkdir('./星巴克')

for index in range(len(img_list)):
    url = img_list[index].attrs['style'].split(r'"')
    url = 'https://www.starbucks.com.cn' + url[1]
    name = name_list[index].get_text()
    name = name.replace(r"/", "-")
    suffix = url.split('.')[-1]

    try:
        urllib.request.urlretrieve(url=url, filename='./星巴克/' + name + '.' + suffix)
    except:
        print(f'我吼-{name}')
        os.remove(os.path.join(os.path.dirname(__file__), '星巴克', name + '.' + suffix))
    print(f'{name}, {url}')

```

