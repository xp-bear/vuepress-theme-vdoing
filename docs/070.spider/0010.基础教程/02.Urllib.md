---
title: Urllibæ¨¡å—
date: 2023-02-07 16:45:50
permalink: /pages/8070d8/
categories:
  - spider
tags:
  - 
---
# Urllib

## 1.ä»€ä¹ˆæ˜¯äº’è”ç½‘çˆ¬è™«ï¼Ÿ

å¦‚æœæˆ‘ä»¬æŠŠäº’è”ç½‘æ¯”ä½œä¸€å¼ å¤§çš„èœ˜è››ç½‘ï¼Œé‚£ä¸€å°è®¡ç®—æœºä¸Šçš„æ•°æ®ä¾¿æ˜¯èœ˜è››ç½‘ä¸Šçš„ä¸€ä¸ªçŒç‰©ï¼Œè€Œçˆ¬è™«ç¨‹åºå°±æ˜¯ä¸€åªå°
èœ˜è››ï¼Œæ²¿ç€èœ˜è››ç½‘æŠ“å–è‡ªå·±æƒ³è¦çš„æ•°æ®

```python
è§£é‡Š1ï¼šé€šè¿‡ä¸€ä¸ªç¨‹åºï¼Œæ ¹æ®Url(http://www.taobao.com)è¿›è¡Œçˆ¬å–ç½‘é¡µï¼Œè·å–æœ‰ç”¨ä¿¡æ¯
è§£é‡Š2ï¼šä½¿ç”¨ç¨‹åºæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œå»å‘æœåŠ¡å™¨å‘é€è¯·æ±‚ï¼Œè·å–å“åº”ä¿¡æ¯
```

## 2.çˆ¬è™«æ ¸å¿ƒ?

```python
1.çˆ¬å–ç½‘é¡µï¼šçˆ¬å–æ•´ä¸ªç½‘é¡µ åŒ…å«äº†ç½‘é¡µä¸­æ‰€æœ‰å¾—å†…å®¹
2.è§£ææ•°æ®ï¼šå°†ç½‘é¡µä¸­ä½ å¾—åˆ°çš„æ•°æ® è¿›è¡Œè§£æ
3.éš¾ç‚¹ï¼šçˆ¬è™«å’Œåçˆ¬è™«ä¹‹é—´çš„åšå¼ˆ
```

## 3.çˆ¬è™«çš„ç”¨é€”ï¼Ÿ

- æ•°æ®åˆ†æ/äººå·¥æ•°æ®é›†
- ç¤¾äº¤è½¯ä»¶å†·å¯åŠ¨
- èˆ†æƒ…ç›‘æ§
- ç«äº‰å¯¹æ‰‹ç›‘æ§

![1642351956496](http://mk.xxoutman.cn/spyder/1642351956496.png)

## 4.çˆ¬è™«åˆ†ç±»ï¼Ÿ

```python
1.é€šç”¨çˆ¬è™«ï¼š
å®ä¾‹
	ç™¾åº¦ã€360ã€googleã€sougouç­‰æœç´¢å¼•æ“â€â€â€ä¼¯ä¹åœ¨çº¿
åŠŸèƒ½
	è®¿é—®ç½‘é¡µâ€>æŠ“å–æ•°æ®â€>æ•°æ®å­˜å‚¨â€>æ•°æ®å¤„ç†â€>æä¾›æ£€ç´¢æœåŠ¡
robotsåè®®
	ä¸€ä¸ªçº¦å®šä¿—æˆçš„åè®®ï¼Œæ·»åŠ robots.txtæ–‡ä»¶ï¼Œæ¥è¯´æ˜æœ¬ç½‘ç«™å“ªäº›å†…å®¹ä¸å¯ä»¥è¢«æŠ“å–ï¼Œå¯¹æˆ‘ä»¬æ¥è¯´èµ·ä¸åˆ°é™åˆ¶ä½œç”¨
	è‡ªå·±å†™çš„çˆ¬è™«æ— éœ€éµå®ˆ
ç½‘ç«™æ’å(SEO)
	1. æ ¹æ®pagerankç®—æ³•å€¼è¿›è¡Œæ’åï¼ˆå‚è€ƒä¸ªç½‘ç«™æµé‡ã€ç‚¹å‡»ç‡ç­‰æŒ‡æ ‡ï¼‰
	2. ç™¾åº¦ç«ä»·æ’å
ç¼ºç‚¹
	1. æŠ“å–çš„æ•°æ®å¤§å¤šæ˜¯æ— ç”¨çš„
	2.ä¸èƒ½æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚æ¥ç²¾å‡†è·å–æ•°æ®
```

```python
2.èšç„¦çˆ¬è™«
åŠŸèƒ½
	æ ¹æ®éœ€æ±‚ï¼Œå®ç°çˆ¬è™«ç¨‹åºï¼ŒæŠ“å–éœ€è¦çš„æ•°æ®
è®¾è®¡æ€è·¯
	1.ç¡®å®šè¦çˆ¬å–çš„url
	å¦‚ä½•è·å–Url
	2.æ¨¡æ‹Ÿæµè§ˆå™¨é€šè¿‡httpåè®®è®¿é—®urlï¼Œè·å–æœåŠ¡å™¨è¿”å›çš„htmlä»£ç 
	å¦‚ä½•è®¿é—®
	3.è§£æhtmlå­—ç¬¦ä¸²ï¼ˆæ ¹æ®ä¸€å®šè§„åˆ™æå–éœ€è¦çš„æ•°æ®ï¼‰
	å¦‚ä½•è§£æ
```

## 5.åçˆ¬æ‰‹æ®µï¼Ÿ

### 1.Userâ€Agent

â€‹	User Agentä¸­æ–‡åä¸ºç”¨æˆ·ä»£ç†ï¼Œç®€ç§° UAï¼Œå®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šå­—ç¬¦ä¸²å¤´ï¼Œä½¿å¾—æœåŠ¡å™¨èƒ½å¤Ÿè¯†åˆ«å®¢æˆ·ä½¿ç”¨çš„æ“ä½œç³»ç»ŸåŠç‰ˆæœ¬ã€CPU ç±»å‹ã€æµè§ˆå™¨åŠç‰ˆæœ¬ã€æµè§ˆå™¨æ¸²æŸ“å¼•æ“ã€æµè§ˆå™¨è¯­è¨€ã€æµè§ˆå™¨æ’ä»¶ç­‰ã€‚

### 2.ä»£ç†IP

â€‹	è¥¿æ¬¡ä»£ç†ã€‚
â€‹	å¿«ä»£ç†ã€‚
â€‹	ä»€ä¹ˆæ˜¯é«˜åŒ¿åã€åŒ¿åå’Œé€æ˜ä»£ç†ï¼Ÿå®ƒä»¬æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

â€‹		1.ä½¿ç”¨é€æ˜ä»£ç†ï¼Œå¯¹æ–¹æœåŠ¡å™¨å¯ä»¥çŸ¥é“ä½ ä½¿ç”¨äº†ä»£ç†ï¼Œå¹¶ä¸”ä¹ŸçŸ¥é“ä½ çš„çœŸå®IPã€‚

â€‹		2.ä½¿ç”¨åŒ¿åä»£ç†ï¼Œå¯¹æ–¹æœåŠ¡å™¨å¯ä»¥çŸ¥é“ä½ ä½¿ç”¨äº†ä»£ç†ï¼Œä½†ä¸çŸ¥é“ä½ çš„çœŸå®IPã€‚

â€‹		3.ä½¿ç”¨é«˜åŒ¿åä»£ç†ï¼Œå¯¹æ–¹æœåŠ¡å™¨ä¸çŸ¥é“ä½ ä½¿ç”¨äº†ä»£ç†ï¼Œæ›´ä¸çŸ¥é“ä½ çš„çœŸå®IPã€‚

### 3.éªŒè¯ç è®¿é—®

â€‹	æ‰“ç å¹³å°
â€‹		å›¾é‰´
â€‹		è¶…çº§é¹°ğŸ¦…

### 4.åŠ¨æ€åŠ è½½ç½‘é¡µ 

â€‹	ç½‘ç«™è¿”å›çš„æ˜¯jsæ•°æ® å¹¶ä¸æ˜¯ç½‘é¡µçš„çœŸå®æ•°æ®ã€‚
â€‹    è§£å†³æ–¹æ¡ˆ: seleniumé©±åŠ¨çœŸå®çš„æµè§ˆå™¨å‘é€è¯·æ±‚ã€‚

### 5.æ•°æ®åŠ å¯†

â€‹	åˆ†æjsä»£ç (jsé€†å‘)

## 6.urllibåº“ä½¿ç”¨

`urllib.request.urlopen()` æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚ã€‚
`response` æœåŠ¡å™¨è¿”å›çš„æ•°æ®ï¼Œresponseçš„æ•°æ®ç±»å‹æ˜¯`HttpResponse`ã€‚

å­—èŠ‚â€â€>å­—ç¬¦ä¸²:
		è§£ç `decode`
å­—ç¬¦ä¸²â€â€>å­—èŠ‚:
		ç¼–ç `encode`

- `read() `	å­—èŠ‚å½¢å¼è¯»å–äºŒè¿›åˆ¶ æ‰©å±•ï¼šread(5)è¿”å›å‰5ä¸ªå­—èŠ‚

- `readline()`  è¯»å–ä¸€è¡Œ

- `readlines()`  ä¸€è¡Œä¸€è¡Œè¯»å– ç›´è‡³ç»“æŸ

- `getcode()`  è·å–çŠ¶æ€ç 

- `geturl()`  è·å–url

- `getheaders()`  è·å–headers

  ```python
    import urllib.request
    url = 'http://www.baidu.com'
  # æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
    response = urllib.request.urlopen(url)
  
  # ä¸€ä¸ªç±»å‹å’Œå…­ä¸ªæ–¹æ³•
  
  # responseæ˜¯HTTPResponseçš„ç±»å‹
  # print(type(response))
  
  # æŒ‰ç…§ä¸€ä¸ªå­—èŠ‚ä¸€ä¸ªå­—èŠ‚çš„å»è¯»
  # content = response.read()
  # print(content)
  
  # è¿”å›å¤šå°‘ä¸ªå­—èŠ‚
  # content = response.read(5)
  # print(content)
  
  # è¯»å–ä¸€è¡Œ
  # content = response.readline()
  # print(content)
  # content = response.readlines()
  # print(content)
  
  # è¿”å›çŠ¶æ€ç   å¦‚æœæ˜¯200äº† é‚£ä¹ˆå°±è¯æ˜æˆ‘ä»¬çš„é€»è¾‘æ²¡æœ‰é”™
  # print(response.getcode())
  
  # è¿”å›çš„æ˜¯urlåœ°å€
  # print(response.geturl())
  
  # è·å–æ˜¯ä¸€ä¸ªå“åº”å¤´çš„çŠ¶æ€ä¿¡æ¯
    print(response.getheaders())
  
  # ä¸€ä¸ªç±»å‹ HTTPResponse
  
  # å…­ä¸ªæ–¹æ³• read  readline  readlines  getcode  geturl getheaders
  ```

- `urllib.request.urlretrieve()` 
    è¯·æ±‚ç½‘é¡µ
    è¯·æ±‚å›¾ç‰‡
    è¯·æ±‚è§†é¢‘
```python
    import urllib.request
    
    # ä¸‹è½½ç½‘é¡µ
    url_page = 'http://www.baidu.com'
    
    # urlä»£è¡¨çš„æ˜¯ä¸‹è½½çš„è·¯å¾„  filenameæ–‡ä»¶çš„åå­—
    # åœ¨pythonä¸­ å¯ä»¥å†™å˜é‡çš„åå­—  ä¹Ÿå¯ä»¥ç›´æ¥å†™å€¼
    urllib.request.urlretrieve(url_page,'baidu.html')
    
    # ä¸‹è½½å›¾ç‰‡
    # url_img = 'https://img1.baidu.com/it/u=3004965690,4089234593&fm=26&fmt=auto&gp=0.jpg'
    #
    # urllib.request.urlretrieve(url= url_img,filename='lisa.jpg')
    
    # ä¸‹è½½è§†é¢‘
    # url_video = 'https://vd3.bdstatic.com/mda-mhkku4ndaka5etk3/1080p/cae_h264/1629557146541497769/mda-mhkku4ndaka5etk3.mp4?v_from_s=hkapp-haokan-tucheng&auth_key=1629687514-0-0-7ed57ed7d1168bb1f06d18a4ea214300&bcevod_channel=searchbox_feed&pd=1&pt=3&abtest='
    #
    # urllib.request.urlretrieve(url_video, 'hxekyyds.mp4')
    
```

## 7.è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶

UAä»‹ç»ï¼šUser Agentä¸­æ–‡åä¸ºç”¨æˆ·ä»£ç†ï¼Œç®€ç§° UAï¼Œå®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šå­—ç¬¦ä¸²å¤´ï¼Œä½¿å¾—æœåŠ¡å™¨èƒ½å¤Ÿè¯†åˆ«å®¢æˆ·ä½¿ç”¨çš„æ“ä½œç³»ç»ŸåŠç‰ˆæœ¬ã€CPU ç±»å‹ã€æµè§ˆå™¨åŠç‰ˆæœ¬ã€‚æµè§ˆå™¨å†…æ ¸ã€æµè§ˆå™¨æ¸²æŸ“å¼•æ“ã€æµè§ˆå™¨è¯­è¨€ã€æµè§ˆå™¨æ’ä»¶ç­‰ã€‚

```python
è¯­æ³•ï¼šrequest = urllib.request.Request()
```

æ‰©å±•ï¼šç¼–ç çš„ç”±æ¥

> ç¼–ç é›†çš„æ¼”å˜
> ç”±äºè®¡ç®—æœºæ˜¯ç¾å›½äººå‘æ˜çš„ï¼Œå› æ­¤ï¼Œæœ€æ—©åªæœ‰127ä¸ªå­—ç¬¦è¢«ç¼–ç åˆ°è®¡ç®—æœºé‡Œï¼Œä¹Ÿå°±æ˜¯å¤§å°å†™è‹±æ–‡å­—æ¯ã€æ•°å­—å’Œä¸€äº›ç¬¦å·ï¼Œè¿™ä¸ªç¼–ç è¡¨è¢«ç§°ä¸ºASCIIç¼–ç ï¼Œæ¯”å¦‚å¤§å†™å­—æ¯Açš„ç¼–ç æ˜¯65ï¼Œå°å†™å­—æ¯zçš„ç¼–ç æ˜¯122ã€‚
> ä½†æ˜¯è¦å¤„ç†ä¸­æ–‡æ˜¾ç„¶ä¸€ä¸ªå­—èŠ‚æ˜¯ä¸å¤Ÿçš„ï¼Œè‡³å°‘éœ€è¦ä¸¤ä¸ªå­—èŠ‚ï¼Œè€Œä¸”è¿˜ä¸èƒ½å’ŒASCIIç¼–ç å†²çªï¼Œæ‰€ä»¥ï¼Œä¸­å›½åˆ¶å®šäº†GB2312ç¼–ç ï¼Œç”¨æ¥æŠŠä¸­æ–‡ç¼–è¿›å»ã€‚
> ä½ å¯ä»¥æƒ³å¾—åˆ°çš„æ˜¯ï¼Œå…¨ä¸–ç•Œæœ‰ä¸Šç™¾ç§è¯­è¨€ï¼Œæ—¥æœ¬æŠŠæ—¥æ–‡ç¼–åˆ°Shift_JISé‡Œï¼ŒéŸ©å›½æŠŠéŸ©æ–‡ç¼–åˆ°Eucâ€kré‡Œï¼Œå„å›½æœ‰å„å›½çš„æ ‡å‡†ï¼Œå°±ä¼šä¸å¯é¿å…åœ°å‡ºç°å†²çªï¼Œç»“æœå°±æ˜¯ï¼Œåœ¨å¤šè¯­è¨€æ··åˆçš„æ–‡æœ¬ä¸­ï¼Œæ˜¾ç¤ºå‡ºæ¥ä¼šæœ‰ä¹±ç ã€‚
> å› æ­¤ï¼ŒUnicodeåº”è¿è€Œç”Ÿã€‚UnicodeæŠŠæ‰€æœ‰è¯­è¨€éƒ½ç»Ÿä¸€åˆ°ä¸€å¥—ç¼–ç é‡Œï¼Œè¿™æ ·å°±ä¸ä¼šå†æœ‰ä¹±ç é—®é¢˜äº†ã€‚
> Unicodeæ ‡å‡†ä¹Ÿåœ¨ä¸æ–­å‘å±•ï¼Œä½†æœ€å¸¸ç”¨çš„æ˜¯ç”¨ä¸¤ä¸ªå­—èŠ‚è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ï¼ˆå¦‚æœè¦ç”¨åˆ°éå¸¸ååƒ»çš„å­—ç¬¦ï¼Œå°±éœ€è¦4ä¸ªå­—èŠ‚ï¼‰ã€‚
> ç°ä»£æ“ä½œç³»ç»Ÿå’Œå¤§å¤šæ•°ç¼–ç¨‹è¯­è¨€éƒ½ç›´æ¥æ”¯æŒUnicodeã€‚

```python
import urllib.request

url = 'https://www.baidu.com'

# urlçš„ç»„æˆ
# https://www.baidu.com/s?wd=å‘¨æ°ä¼¦

# http/https    www.baidu.com   80/443        s      wd = å‘¨æ°ä¼¦     #
#    åè®®             ä¸»æœº        ç«¯å£å·     è·¯å¾„     å‚æ•°           é”šç‚¹
# http   80
# https  443
# mysql  3306
# oracle 1521
# redis  6379
# mongodb 27017

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

# å› ä¸ºurlopenæ–¹æ³•ä¸­ä¸èƒ½å­˜å‚¨å­—å…¸ æ‰€ä»¥headersä¸èƒ½ä¼ é€’è¿›å»
# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
# æ³¨æ„å› ä¸ºå‚æ•°é¡ºåºçš„é—®é¢˜ ä¸èƒ½ç›´æ¥å†™urlå’Œheaders ä¸­é—´è¿˜æœ‰dataæ‰€ä»¥æˆ‘ä»¬éœ€è¦å…³é”®å­—ä¼ å‚
request = urllib.request.Request(url=url, headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode('utf8')

print(content)
```

## 8.ç¼–è§£ç 

### 1.getè¯·æ±‚æ–¹å¼ï¼šurllib.parse.quoteï¼ˆï¼‰

```python
# https://www.baidu.com/s?wd=%E5%91%A8%E6%9D%B0%E4%BC%A6

# éœ€æ±‚ è·å– https://www.baidu.com/s?wd=å‘¨æ°ä¼¦çš„ç½‘é¡µæºç 

import urllib.request
import urllib.parse

url = 'https://www.baidu.com/s?wd='

# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶ä¸ºäº†è§£å†³åçˆ¬çš„ç¬¬ä¸€ç§æ‰‹æ®µ
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

# å°†å‘¨æ°ä¼¦ä¸‰ä¸ªå­—å˜æˆunicodeç¼–ç çš„æ ¼å¼
# æˆ‘ä»¬éœ€è¦ä¾èµ–äºurllib.parse
name = urllib.parse.quote('å‘¨æ°ä¼¦')
print(name)
url = url + name

# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request = urllib.request.Request(url=url, headers=headers)

# æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
response = urllib.request.urlopen(request)

# è·å–å“åº”çš„å†…å®¹
content = response.read().decode('utf-8')

# æ‰“å°æ•°æ®
print(content)
```

### 2.getè¯·æ±‚æ–¹å¼ï¼šurllib.parse.urlencodeï¼ˆï¼‰

```python
# urlencodeåº”ç”¨åœºæ™¯ï¼šå¤šä¸ªå‚æ•°çš„æ—¶å€™


# https://www.baidu.com/s?wd=å‘¨æ°ä¼¦&sex=ç”·

# import urllib.parse

#
# data = {
#     'wd': 'å‘¨æ°ä¼¦',
#     'sex': 'ç”·',
#     'location': 'ä¸­å›½å°æ¹¾çœ'
# }
# #
# a = urllib.parse.urlencode(data)
# print(a) 
#è¾“å‡ºå‚æ•°çš„æ‹¼æ¥ wd=%E5%91%A8%E6%9D%B0%E4%BC%A6&sex=%E7%94%B7&location=%E4%B8%AD%E5%9B%BD%E5%8F%B0%E6%B9%BE%E7%9C%81

# https://www.baidu.com/s?wd=å‘¨æ°ä¼¦&sex=ç”· çš„ç½‘é¡µæºç 
# è·å–https://www.baidu.com/s?wd=%E5%91%A8%E6%9D%B0%E4%BC%A6&sex=%E7%94%B7

import urllib.request
import urllib.parse

base_url = 'https://www.baidu.com/s?'

data = {
    'wd': 'å‘¨æ°ä¼¦',
    'sex': 'ç”·',
    'location': 'ä¸­å›½å°æ¹¾çœ'
}

new_data = urllib.parse.urlencode(data)

# è¯·æ±‚èµ„æºè·¯å¾„
url = base_url + new_data

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request = urllib.request.Request(url=url, headers=headers)

# æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
response = urllib.request.urlopen(request)

# è·å–ç½‘é¡µæºç çš„æ•°æ®
content = response.read().decode('utf-8')

# æ‰“å°æ•°æ®
print(content)
```

### 3.postè¯·æ±‚æ–¹å¼

```python
# postè¯·æ±‚

import urllib.request
import urllib.parse

url = 'https://fanyi.baidu.com/sug'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

data = {
    'kw': 'spider'
}

# postè¯·æ±‚çš„å‚æ•° å¿…é¡»è¦è¿›è¡Œç¼–ç 
data = urllib.parse.urlencode(data).encode('utf-8')

# postçš„è¯·æ±‚çš„å‚æ•° æ˜¯ä¸ä¼šæ‹¼æ¥åœ¨urlçš„åé¢çš„  è€Œæ˜¯éœ€è¦æ”¾åœ¨è¯·æ±‚å¯¹è±¡å®šåˆ¶çš„å‚æ•°ä¸­
# postè¯·æ±‚çš„å‚æ•° å¿…é¡»è¦è¿›è¡Œç¼–ç 
request = urllib.request.Request(url=url, data=data, headers=headers)

# æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
response = urllib.request.urlopen(request)

# è·å–å“åº”çš„æ•°æ®
content = response.read().decode('utf-8')
print(content)  # [{"k":"spider","v":"n. \u8718\u86db; \u661f\u5f62\u8f6e\uff0c\u5341\u5b57\u53c9;...
print(type(content))  # <class 'str'>
# å­—ç¬¦ä¸² --> jsonå¯¹è±¡

import json

obj = json.loads(content)
print(obj)

# postè¯·æ±‚æ–¹å¼çš„å‚æ•° å¿…é¡»ç¼–ç    data = urllib.parse.urlencode(data)
# ç¼–ç ä¹‹å å¿…é¡»è°ƒç”¨encodeæ–¹æ³• data = urllib.parse.urlencode(data).encode('utf-8')
# å‚æ•°æ˜¯æ”¾åœ¨è¯·æ±‚å¯¹è±¡å®šåˆ¶çš„æ–¹æ³•ä¸­  request = urllib.request.Request(url=url,data=data,headers=headers)
```

æ€»ç»“ï¼špostå’ŒgetåŒºåˆ«ï¼Ÿ

1ï¼šgetè¯·æ±‚æ–¹å¼çš„å‚æ•°å¿…é¡»ç¼–ç ï¼Œå‚æ•°æ˜¯æ‹¼æ¥åˆ°urlåé¢ï¼Œç¼–ç ä¹‹åä¸éœ€è¦è°ƒç”¨encodeæ–¹æ³•
2ï¼špostè¯·æ±‚æ–¹å¼çš„å‚æ•°å¿…é¡»ç¼–ç  ï¼Œå‚æ•°æ˜¯æ”¾åœ¨è¯·æ±‚å¯¹è±¡å®šåˆ¶çš„æ–¹æ³•ä¸­ï¼Œç¼–ç ä¹‹åéœ€è¦è°ƒç”¨encodeæ–¹æ³•

æ¡ˆä¾‹ç»ƒä¹ ï¼šç™¾åº¦ç¿»è¯‘ä¹‹è¯¦ç»†ç¿»è¯‘

```python
import urllib.request
import urllib.parse

url = 'https://fanyi.baidu.com/v2transapi?from=en&to=zh'

headers = {
    # 'Accept': '*/*',
    # 'Accept-Encoding': 'gzip, deflate, br',
    # 'Accept-Language': 'zh-CN,zh;q=0.9',
    # 'Connection': 'keep-alive',
    # 'Content-Length': '135',
    # 'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'Cookie': 'BIDUPSID=DAA8F9F0BD801A2929D96D69CF7EBF50; PSTM=1597202227; BAIDUID=DAA8F9F0BD801A29B2813502000BF8E9:SL=0:NR=10:FG=1; __yjs_duid=1_c19765bd685fa6fa12c2853fc392f8db1618999058029; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; BDUSS=R2bEZvTjFCNHQxdUV-cTZ-MzZrSGxhbUYwSkRkUWk2SkxxS3E2M2lqaFRLUlJoRVFBQUFBJCQAAAAAAAAAAAEAAAA3e~BTveK-9sHLZGF5AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFOc7GBTnOxgaW; BDUSS_BFESS=R2bEZvTjFCNHQxdUV-cTZ-MzZrSGxhbUYwSkRkUWk2SkxxS3E2M2lqaFRLUlJoRVFBQUFBJCQAAAAAAAAAAAEAAAA3e~BTveK-9sHLZGF5AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFOc7GBTnOxgaW; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; BAIDUID_BFESS=DAA8F9F0BD801A29B2813502000BF8E9:SL=0:NR=10:FG=1; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; PSINO=2; H_PS_PSSID=34435_31660_34405_34004_34073_34092_26350_34426_34323_22158_34390; delPer=1; BA_HECTOR=8185a12020018421b61gi6ka20q; BCLID=10943521300863382545; BDSFRCVID=boDOJexroG0YyvRHKn7hh7zlD_weG7bTDYLEOwXPsp3LGJLVJeC6EG0Pts1-dEu-EHtdogKK0mOTHv8F_2uxOjjg8UtVJeC6EG0Ptf8g0M5; H_BDCLCKID_SF=tR3aQ5rtKRTffjrnhPF3-44vXP6-hnjy3bRkX4Q4Wpv_Mnndjn6SQh4Wbttf5q3RymJ42-39LPO2hpRjyxv4y4Ldj4oxJpOJ-bCL0p5aHl51fbbvbURvD-ug3-7qqU5dtjTO2bc_5KnlfMQ_bf--QfbQ0hOhqP-jBRIE3-oJqC8hMIt43f; BCLID_BFESS=10943521300863382545; BDSFRCVID_BFESS=boDOJexroG0YyvRHKn7hh7zlD_weG7bTDYLEOwXPsp3LGJLVJeC6EG0Pts1-dEu-EHtdogKK0mOTHv8F_2uxOjjg8UtVJeC6EG0Ptf8g0M5; H_BDCLCKID_SF_BFESS=tR3aQ5rtKRTffjrnhPF3-44vXP6-hnjy3bRkX4Q4Wpv_Mnndjn6SQh4Wbttf5q3RymJ42-39LPO2hpRjyxv4y4Ldj4oxJpOJ-bCL0p5aHl51fbbvbURvD-ug3-7qqU5dtjTO2bc_5KnlfMQ_bf--QfbQ0hOhqP-jBRIE3-oJqC8hMIt43f; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1629701482,1629702031,1629702343,1629704515; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1629704515; __yjs_st=2_MDBkZDdkNzg4YzYyZGU2NTM5NzBjZmQ0OTZiMWRmZGUxM2QwYzkwZTc2NTZmMmIxNDJkYzk4NzU1ZDUzN2U3Yjc4ZTJmYjE1YTUzMTljYWFkMWUwYmVmZGEzNmZjN2FlY2M3NDAzOThhZTY5NzI0MjVkMmQ0NWU3MWE1YTJmNGE5NDBhYjVlOWY3MTFiMWNjYTVhYWI0YThlMDVjODBkNWU2NjMwMzY2MjFhZDNkMzVhNGMzMGZkMWY2NjU5YzkxMDk3NTEzODJiZWUyMjEyYTk5YzY4ODUyYzNjZTJjMGM5MzhhMWE5YjU3NTM3NWZiOWQxNmU3MDVkODExYzFjN183XzliY2RhYjgz; ab_sr=1.0.1_ZTc2ZDFkMTU5ZTM0ZTM4MWVlNDU2MGEzYTM4MzZiY2I2MDIxNzY1Nzc1OWZjZGNiZWRhYjU5ZjYwZmNjMTE2ZjIzNmQxMTdiMzIzYTgzZjVjMTY0ZjM1YjMwZTdjMjhiNDRmN2QzMjMwNWRhZmUxYTJjZjZhNTViMGM2ODFlYjE5YTlmMWRjZDAwZGFmMDY4ZTFlNGJiZjU5YzE1MGIxN2FiYTU3NDgzZmI4MDdhMDM5NTQ0MjQxNDBiNzdhMDdl',
    # 'Host': 'fanyi.baidu.com',
    # 'Origin': 'https://fanyi.baidu.com',
    # 'Referer': 'https://fanyi.baidu.com/?aldtype=16047',
    # 'sec-ch-ua': '"Chromium";v="92", " Not A;Brand";v="99", "Google Chrome";v="92"',
    # 'sec-ch-ua-mobile': '?0',
    # 'Sec-Fetch-Dest': 'empty',
    # 'Sec-Fetch-Mode': 'cors',
    # 'Sec-Fetch-Site': 'same-origin',
    # 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
    # 'X-Requested-With': 'XMLHttpRequest',
}

data = {
    'from': 'en',
    'to': 'zh',
    'query': 'love',
    'transtype': 'realtime',
    'simple_means_flag': '3',
    'sign': '198772.518981',
    'token': '5483bfa652979b41f9c90d91f3de875d',
    'domain': 'common',
}

# postè¯·æ±‚çš„å‚æ•°  å¿…é¡»è¿›è¡Œç¼–ç  å¹¶ä¸”è¦è°ƒç”¨encodeæ–¹æ³•
data = urllib.parse.urlencode(data).encode('utf-8')

# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request = urllib.request.Request(url=url, data=data, headers=headers)

# æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
response = urllib.request.urlopen(request)

# è·å–å“åº”çš„æ•°æ®
content = response.read().decode('utf-8')

import json

obj = json.loads(content)
print(obj) #{'trans_result': {'data': [{'dst': 'çˆ±', 'prefixWrap': 0, 'result': [[0, 'çˆ±', ['0|4'], [], ['0|4'], ['0|3']]], 'src': 'love'}],...
```

## 9.ajaxçš„getè¯·æ±‚

æ¡ˆä¾‹ï¼šè±†ç“£ç”µå½±(https://movie.douban.com/typerank?type_name=%E5%8A%A8%E4%BD%9C&type=5&interval_id=100:90&action=)

ajaxçš„getè¯·æ±‚è±†ç“£ç”µå½±ç¬¬ä¸€é¡µ

```python
# getè¯·æ±‚
# è·å–è±†ç“£ç”µå½±çš„ç¬¬ä¸€é¡µçš„æ•°æ® å¹¶ä¸”ä¿å­˜èµ·æ¥

import urllib.request

url = 'https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&start=0&limit=20'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

# (1) è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request = urllib.request.Request(url=url, headers=headers)

# ï¼ˆ2ï¼‰è·å–å“åº”çš„æ•°æ®
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')

# (3) æ•°æ®ä¸‹è½½åˆ°æœ¬åœ°
# openæ–¹æ³•é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨çš„æ˜¯gbkçš„ç¼–ç   å¦‚æœæˆ‘ä»¬è¦æƒ³ä¿å­˜æ±‰å­— é‚£ä¹ˆéœ€è¦åœ¨openæ–¹æ³•ä¸­æŒ‡å®šç¼–ç æ ¼å¼ä¸ºutf-8
# encoding = 'utf-8'
# fp = open('douban.json','w',encoding='utf-8')
# fp.write(content)

with open('./douban.json', 'w', encoding='utf-8') as fp:
    fp.write(content)
```

ajaxçš„getè¯·æ±‚çˆ¬å–è±†ç“£ç”µå½±å‰10é¡µæ•°æ®

```python
# çˆ¬å–è±†ç“£ç”µå½±å‰10é¡µæ•°æ®
# https://movie.douban.com/j/chart/top_list?type=20&interval_id=100%3A90&action=&start=0&limit=20
# https://movie.douban.com/j/chart/top_list?type=20&interval_id=100%3A90&action=&start=20&limit=20
# https://movie.douban.com/j/chart/top_list?type=20&interval_id=100%3A90&action=&start=40&limit=20
import urllib.request
import urllib.parse
# ä¸‹è½½å‰10é¡µæ•°æ®
# ä¸‹è½½çš„æ­¥éª¤ï¼š1.è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶ 2.è·å–å“åº”çš„æ•°æ® 3.ä¸‹è½½
# æ¯æ‰§è¡Œä¸€æ¬¡è¿”å›ä¸€ä¸ªrequestå¯¹è±¡
def create_request(page):
    base_url = 'https://movie.douban.com/j/chart/top_list?type=20&interval_id=100%3A90&action=&'
    headers = {
        'Userâ€Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML,like Gecko) Chrome/76.0.3809.100 Safari/537.36'
    }
    data={
        # 1 2 3 4
        # 0 20 40 60
        'start':(pageâ€1)*20,
        'limit':20
    }
    # dataç¼–ç 
    data = urllib.parse.urlencode(data)
    url = base_url + data
    request = urllib.request.Request(url=url,headers=headers)
    return request

# è·å–ç½‘é¡µæºç 
def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utfâ€8')
    return content

def down_load(page,content):
    # with openï¼ˆæ–‡ä»¶çš„åå­—ï¼Œæ¨¡å¼ï¼Œç¼–ç ï¼‰as fp:
    # fp.write(å†…å®¹)
    with open('douban_'+str(page)+'.json','w',encoding='utfâ€8')as fp:
    fp.write(content)
    
if __name__ == '__main__':
    start_page = int(input('è¯·è¾“å…¥èµ·å§‹é¡µç '))
    end_page = int(input('è¯·è¾“å…¥ç»“æŸé¡µç '))
    for page in range(start_page,end_page+1):
        request = create_request(page)
        content = get_content(request)
        down_load(page,content)
```

## 10.ajaxçš„postè¯·æ±‚

æ¡ˆä¾‹ï¼šKFCå®˜ç½‘(http://www.kfc.com.cn/kfccda/storelist/index.aspx)

```python
# 1é¡µ
# http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname
# post
# cname: åŒ—äº¬
# pid:
# pageIndex: 1
# pageSize: 10


# 2é¡µ
# http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname
# post
# cname: åŒ—äº¬
# pid:
# pageIndex: 2
# pageSize: 10

import urllib.request
import urllib.parse


def create_request(page):
    base_url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'

    data = {
        'cname': 'æ­¦æ±‰',
        'pid': '',
        'pageIndex': page,
        'pageSize': '10'
    }

    data = urllib.parse.urlencode(data).encode('utf-8')

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
    }

    request = urllib.request.Request(url=base_url, headers=headers, data=data)

    return request


def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content


def down_load(page, content):
    with open('kfc_' + str(page) + '.json', 'w', encoding='utf-8')as fp:
        fp.write(content)


if __name__ == '__main__':
    start_page = int(input('è¯·è¾“å…¥èµ·å§‹é¡µç '))
    end_page = int(input('è¯·è¾“å…¥ç»“æŸé¡µç '))

    for page in range(start_page, end_page + 1):
        # è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
        request = create_request(page)
        # è·å–ç½‘é¡µæºç 
        content = get_content(request)
        # ä¸‹è½½
        down_load(page, content)
```

## 11.URLError\HTTPError

ç®€ä»‹: 

1.HTTPErrorç±»æ˜¯URLErrorç±»çš„å­ç±»ã€‚

2.å¯¼å…¥çš„åŒ… `urllib.error.HTTPError`   `urllib.error.URLError`ã€‚

3.httpé”™è¯¯ï¼šhttpé”™è¯¯æ˜¯é’ˆå¯¹æµè§ˆå™¨æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨è€Œå¢åŠ å‡ºæ¥çš„é”™è¯¯æç¤ºã€‚å¼•å¯¼å¹¶å‘Šè¯‰æµè§ˆè€…è¯¥é¡µé¢æ˜¯å“ªé‡Œå‡ºäº†é—®é¢˜ã€‚

4.é€šè¿‡urllibå‘é€è¯·æ±‚çš„æ—¶å€™ï¼Œæœ‰å¯èƒ½ä¼šå‘é€å¤±è´¥ï¼Œè¿™ä¸ªæ—¶å€™å¦‚æœæƒ³è®©ä½ çš„ä»£ç æ›´åŠ çš„å¥å£®ï¼Œå¯ä»¥é€šè¿‡tryâ€exceptè¿›è¡Œæ•è·å¼‚å¸¸ï¼Œ
  å¼‚å¸¸æœ‰ä¸¤ç±»ï¼ŒURLError\HTTPErrorã€‚

HTTPErroræ˜¯URLErrorçš„å­ç±»ï¼Œä»–çš„å¼‚å¸¸æœ‰3ä¸ªå±æ€§ï¼š

- code:è¿”å›çŠ¶æ€ç 404è¡¨ç¤ºä¸å­˜åœ¨ï¼Œ500è¡¨ç¤ºæœåŠ¡å™¨é”™è¯¯
- reason:è¿”å›é”™è¯¯åŸå› 
- headers:è¿”å›è¯·æ±‚å¤´

```python
import urllib.request
import urllib.error

url = 'http://www.doudan1111.com'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

try:
    request = urllib.request.Request(url=url, headers=headers)

    response = urllib.request.urlopen(request)

    content = response.read().decode('utf-8')

    print(content)
except urllib.error.HTTPError:
    print('ç³»ç»Ÿæ­£åœ¨å‡çº§ã€‚ã€‚ã€‚')
except urllib.error.URLError:
    print('æˆ‘éƒ½è¯´äº† ç³»ç»Ÿæ­£åœ¨å‡çº§ã€‚ã€‚ã€‚')
```

## 12.cookieç™»å½•

qqç©ºé—´ç™»å½•:

```python
# é€‚ç”¨çš„åœºæ™¯ï¼šæ•°æ®é‡‡é›†çš„æ—¶å€™ éœ€è¦ç»•è¿‡ç™»é™† ç„¶åè¿›å…¥åˆ°æŸä¸ªé¡µé¢
# ä¸ªäººä¿¡æ¯é¡µé¢æ˜¯utf-8  ä½†æ˜¯è¿˜æŠ¥é”™äº†ç¼–ç é”™è¯¯  å› ä¸ºå¹¶æ²¡æœ‰è¿›å…¥åˆ°ä¸ªäººä¿¡æ¯é¡µé¢ è€Œæ˜¯è·³è½¬åˆ°äº†ç™»é™†é¡µé¢
# é‚£ä¹ˆç™»é™†é¡µé¢ä¸æ˜¯utf-8  æ‰€ä»¥æŠ¥é”™

# ä»€ä¹ˆæƒ…å†µä¸‹è®¿é—®ä¸æˆåŠŸï¼Ÿ
# å› ä¸ºè¯·æ±‚å¤´çš„ä¿¡æ¯ä¸å¤Ÿ  æ‰€ä»¥è®¿é—®ä¸æˆåŠŸ

import urllib.request

# qqç©ºé—´é¡µé¢
url = 'https://user.qzone.qq.com/1693889638'

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',
    "cookie": "pac_uid=0_67071cf6d5611; iip=0; pgv_pvid=5394955464; ptui_loginuin=1693889638; RK=Jt+Z402AcZ; ptcz=24bb59a9d3448d849c597db3037ea41238a9a5d9c0fd1fd6e92d50d024ffa172; luin=o1218357610; eas_sid=q1t6e435E562A1n8o208B6P7U5; o_cookie=1218357610; tvfe_boss_uuid=5dd513e15648495d; lskey=000100009c2ef57bef9a8e73b426deae7e7d71dbb646a64cc1578a84f1610a34c3159cf935eb483a0398fcf2; Loading=Yes; pgv_info=ssid=s865660864; _qpsvr_localtk=0.5880324295983725; uin=o1693889638; skey=@MGHOV73Dx; p_uin=o1693889638; pt4_token=cdjl8W4h4-7fShM-j9LhWcY4hv9Aq1EBrR4t7SmOpIU_; p_skey=5*qTgOdzljMhVH3EZjo20LLv6zOHfSwPPL1-KMmMJy8_",
    "referer": "https://user.qzone.qq.com"
}
# è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶
request = urllib.request.Request(url=url, headers=headers)
# æ¨¡æ‹Ÿæµè§ˆå™¨å‘æœåŠ¡å™¨å‘é€è¯·æ±‚
response = urllib.request.urlopen(request)
# è·å–å“åº”çš„æ•°æ®
content = response.read().decode('utf-8')

# å°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ°
with open('qq.html', 'w', encoding='utf-8')as fp:
    fp.write(content)
```

## 13.Handlerå¤„ç†å™¨

ä¸ºä»€ä¹ˆè¦å­¦ä¹ handlerï¼Ÿ

- â€‹    `urllib.request.urlopen(url) `ä¸èƒ½å®šåˆ¶è¯·æ±‚å¤´
- â€‹    `urllib.request.Request(url,headers,data) `å¯ä»¥å®šåˆ¶è¯·æ±‚å¤´
- â€‹    `Handler` å®šåˆ¶æ›´é«˜çº§çš„è¯·æ±‚å¤´
- â€‹    éšç€ä¸šåŠ¡é€»è¾‘çš„å¤æ‚ è¯·æ±‚å¯¹è±¡çš„å®šåˆ¶å·²ç»æ»¡è¶³ä¸äº†æˆ‘ä»¬çš„éœ€æ±‚ï¼Œä¸»è¦æ˜¯ç”¨æ¥åšä»£ç†æœåŠ¡å™¨

```python
#ä¾‹å­: 
# éœ€æ±‚ ä½¿ç”¨handleræ¥è®¿é—®ç™¾åº¦  è·å–ç½‘é¡µæºç 

import urllib.request

url = 'http://www.baidu.com/s?wd=ip'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)

# handler   build_opener  open

# ï¼ˆ1ï¼‰è·å–hanlderå¯¹è±¡
handler = urllib.request.HTTPHandler()

# ï¼ˆ2ï¼‰è·å–openerå¯¹è±¡
opener = urllib.request.build_opener(handler)

# (3) è°ƒç”¨openæ–¹æ³•
response = opener.open(request)

content = response.read().decode('utf-8')

print(content)
with open('handle.html', 'w', encoding='utf-8') as f:
    f.write(content)
```

## 14.ä»£ç†æœåŠ¡å™¨

### 1.ä»£ç†çš„å¸¸ç”¨åŠŸèƒ½?

- 1.çªç ´è‡ªèº«IPè®¿é—®é™åˆ¶ï¼Œè®¿é—®å›½å¤–ç«™ç‚¹ã€‚
- 2.è®¿é—®ä¸€äº›å•ä½æˆ–å›¢ä½“å†…éƒ¨èµ„æºã€‚æ‰©å±•ï¼šæŸå¤§å­¦FTP(å‰ææ˜¯è¯¥ä»£ç†åœ°å€åœ¨è¯¥èµ„æºçš„å…è®¸è®¿é—®èŒƒå›´ä¹‹å†…)ï¼Œä½¿ç”¨æ•™è‚²ç½‘å†…åœ°å€æ®µå…è´¹ä»£ç†æœåŠ¡å™¨ï¼Œå°±å¯ä»¥ç”¨äºå¯¹æ•™è‚²ç½‘å¼€æ”¾çš„å„ç±»FTPä¸‹è½½ä¸Šä¼ ï¼Œä»¥åŠå„ç±»èµ„æ–™æŸ¥è¯¢å…±äº«ç­‰æœåŠ¡ã€‚
- 3.æé«˜è®¿é—®é€Ÿåº¦ã€‚æ‰©å±•ï¼šé€šå¸¸ä»£ç†æœåŠ¡å™¨éƒ½è®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„ç¡¬ç›˜ç¼“å†²åŒºï¼Œå½“æœ‰å¤–ç•Œçš„ä¿¡æ¯é€šè¿‡æ—¶ï¼ŒåŒæ—¶ä¹Ÿå°†å…¶ä¿å­˜åˆ°ç¼“å†²åŒºä¸­ï¼Œå½“å…¶ä»–ç”¨æˆ·å†è®¿é—®ç›¸åŒçš„ä¿¡æ¯æ—¶ï¼Œ åˆ™ç›´æ¥ç”±ç¼“å†²åŒºä¸­å–å‡ºä¿¡æ¯ï¼Œä¼ ç»™ç”¨æˆ·ï¼Œä»¥æé«˜è®¿é—®é€Ÿåº¦ã€‚
- 4.éšè—çœŸå®IPã€‚æ‰©å±•ï¼šä¸Šç½‘è€…ä¹Ÿå¯ä»¥é€šè¿‡è¿™ç§æ–¹æ³•éšè—è‡ªå·±çš„IPï¼Œå…å—æ”»å‡»ã€‚

### 2.ä»£ç é…ç½®ä»£ç†

- åˆ›å»ºReuqestå¯¹è±¡
- åˆ›å»ºProxyHandlerå¯¹è±¡
- ç”¨handlerå¯¹è±¡åˆ›å»ºopenerå¯¹è±¡
- ä½¿ç”¨opener.openå‡½æ•°å‘é€è¯·æ±‚

```python
# éœ€æ±‚ ä½¿ç”¨handleræ¥è®¿é—®ç™¾åº¦  è·å–ç½‘é¡µæºç 

import urllib.request

url = 'http://www.baidu.com/s?wd=ip'

headers = {
    #  cookieä½ å…ˆè‡ªå·±ç™»å½•ç™¾åº¦å¸å·å°±æœ‰äº†
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Cookie': 'BD_UPN=12314753; PSTM=1645619138; BAIDUID=6C26F1D768187F46FEF333F9BF4FD5BE:FG=1; BIDUPSID=9A17F4A46C16A4AC52CBC27ACA53A0DF; BDUSS=Wt0QWlrMkZVeXN2VW1pSGU4QWhHMklrUWNRWHdKUzV4UFd1S0Zucm9xc1REejlpRVFBQUFBJCQAAAAAAAAAAAEAAABwGEfp19-5~dHH1t61xNDcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABOCF2ITghdiV; BDUSS_BFESS=Wt0QWlrMkZVeXN2VW1pSGU4QWhHMklrUWNRWHdKUzV4UFd1S0Zucm9xc1REejlpRVFBQUFBJCQAAAAAAAAAAAEAAABwGEfp19-5~dHH1t61xNDcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABOCF2ITghdiV; __yjs_duid=1_0d031367a111e1984cadc96761213f231645776147631; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; channel=baidusearch; BAIDUID_BFESS=6C26F1D768187F46FEF333F9BF4FD5BE:FG=1; delPer=0; BD_CK_SAM=1; PSINO=6; sug=3; sugstore=1; ORIGIN=2; bdime=0; baikeVisitId=b3660019-7abe-4bc8-bdda-8d22d956f633; H_PS_PSSID=35839_35105_31660_35832_34813_34584_35949_35955_35319_26350_35881_35878_35940; H_PS_645EC=6a75zgMPRmCjweLYDZjpzIfPXqtZP7eeKGjlxKz5Z7LtYBCmVpsNAef78MY; shifen[6847014_97197]=1646897210; BCLID=10772530509546581620; BCLID_BFESS=10772530509546581620; BA_HECTOR=2l250h802kak25a4kn1h2ja1q0r; COOKIE_SESSION=30_2_3_5_8_4_0_0_3_2_1_1_4001_0_0_0_1646895328_1646826219_1646897209%7C9%2315449_39_1646826217%7C9; BD_HOME=1; BDSFRCVID=xzDsJeCCxG3_WP5D2sLYyb9ZsMIRY2CFAFHL3J; H_BDCLCKID_SF=tJ4toCPMJI_3fP36q45HMt00qxby26nKanR9aJ5nQI5nhU7505oUDRtTbb8J0UAJ5m3i2DTvQUbmjRO206oay6O3LlO83h5wW57KKl0MLPbcep68LxODy6DI0xnMBMnr52OnaU513fAKftnOM46JehL3346-35543bRTLnLy5KJYMDF4D5_ae5O3DGRf-b-X25TKBRbq2RREKROvhjRW2x0yyxom3bvxtaOUbI5gttOdotTPeq65XfFwWH5m0folWgteaDcJ-J8XhD-4j6rP; BDSFRCVID_BFESS=xzDsJeCCxG3_WP5D2sLYyb9ZsMIRY2CFAFHL3J; H_BDCLCKID_SF_BFESS=tJ4toCPMJI_3fP36q45HMt00qxby26nKanR9aJ5nQI5nhU7505oUDRtTbb8J0UAJ5m3i2DTvQUbmjRO206oay6O3LlO83h5wW57KKl0MLPbcep68LxODy6DI0xnMBMnr52OnaU513fAKftnOM46JehL3346-35543bRTLnLy5KJYMDF4D5_ae5O3DGRf-b-X25TKBRbq2RREKROvhjRW2x0yyxom3bvxtaOUbI5gttOdotTPeq65XfFwWH5m0folWgteaDcJ-J8XhD-4j6rP; BDSVRTM=74',
}

request = urllib.request.Request(url=url, headers=headers)

# handler   build_opener  open

# ä»£ç†
proxy = {
    'http': "123.180.209.48:4231"
}
# è·å–hanlderå¯¹è±¡
handler = urllib.request.ProxyHandler(proxies=proxy)
# ï¼ˆ2ï¼‰è·å–openerå¯¹è±¡
opener = urllib.request.build_opener(handler)

# (3) è°ƒç”¨openæ–¹æ³•
response = opener.open(request)

content = response.read().decode('utf-8')

print(content)
with open('daili.html', 'w', encoding='utf-8') as f:
    f.write(content)
```

æ‰©å±•ï¼š

- å¿«ä»£ç†
- èŠéº»HTTPä»£ç†
- é’æœç½‘

### 3.ä»£ç†æ± 

```python
import random

pools = [
    {"http": "150.158.21.251:8080"},
    {"http": "150.158.21.251:8090"},
    {"http": "150.158.21.251:90"},
    {"http": "150.158.21.251:80"},
    {"http": "150.158.21.251:00"},
]
result = random.choice(pools)

print(result)
```

